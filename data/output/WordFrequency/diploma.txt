Text pre-processing
     2. Abstractive algorithms
     4. Text summarization datasets
     6. Text pre-processing	
1.1. Removing punctuation and stop words	
1.2. Extractive algorithms	
2.1. Word Frequency	
2.2. Abstractive algorithms
     3.1. Text summarization datasets	
     5.1. generating entirely new words and sentences that did not exist in the source text. PRE-PROCESSING
     The basic principle of most extractive summarization algorithms is that they look for key words to identify which sentences are the most suitable to be extracted for a summary. “at”, “on”, “in”), and even some common lexical words (e.g. Depending on the task, these tokens can be sentences, words, characters, and subwords. Tokenization thefore will be classified into 3 types – sentence, word, and subword (n-gram) tokenization. The code of text summarization algorithms in this study will explicitly use only sentence and word tokenization, but that does not mean subword tokenization does not find its use implicitly (e.g. hidden in a stemming algorithm). Word tokenization is employed in both extractive and abstractive summarization algorithms, to extract key words and generate a vocabulary of a document or a collection of documents. Code for sentence and word tokenization using NLTK:
 

1.3. -s, -es, -’s, -ing) of a word. EXTRACTIVE ALGORITHMS
2.1. Word Frequency
     Word Frequency algorithm is one of the most basic extractive summarization algorithms, and also makes up the first version of the author’s text summarization program. In the second step, the word weights are used to calculate the score of every sentence in the document. The more “important” (higher-weighted) words a sentence has, the higher its score is, and the likelier it is to be included in the final summary. TF-IDF
     Calculating the relevance of a word based on just how common it is in a document can be unreliable. It equates to the logarithm of the number of documents in a collection over the number of documents where the word occurs. After that, sentence scores will be calculated as the sum of TF-IDF values of words in the sentence. Universal Sentence Embeddings encode paragraphs, sentences, and words into numerical vectors. This will be useful for calculating similarity between sentences. [10]
     Below are the steps of the text summarization algorithm using LSA: 
     1. ABSTRACTIVE ALGORITHMS
     The task of abstractive text summarization is to paraphrase paragraphs and generate entirely new sentences out of words that may not exist in the original document. Random sampling, 1-search or n-search is performed in each timestep to find the words that will be used to create the summary. TEXT SUMMARIZATION DATASETS
5.1. Summary generated by Word Frequency:
Where are you settling in your life right now? Word Frequency	TF-IDF	TextRank with USE
The algorithm selects words that are important for the context. Fewer words are selected that are not important for context;
suitable for multi-document summarization. Sometimes chosen words are not important for the context. Synonyms of words are counted as words with the same meaning.