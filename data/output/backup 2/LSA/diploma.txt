MINISTRY OF EDUCATION AND SCIENCE OF THE REPUBLIC OF
KAZAKHSTAN
â€œKazakh-British Technical Universityâ€ JSC
Faculty of Information Technology
Speciality 5B070300 â€œInformation Systemsâ€
ADMITTED TO DEFENCE
Dean of FIT,
Associate Professor, PhD
___________ R. Suliyev
â€œ___â€___________ 2021


EXPLANATORY NOTE
TO GRADUATION PROJECT (work)
Theme: â€œAutomatic summarization of articlesâ€

 
Authors: 
____________ A. Myrzash
â€œ___â€____________ 2021
Major: 
5B070300  
â€œInformation Systemsâ€

Supervisor:
Senior-lecturer
______________ B. Mukhsimbayev
â€œ___â€____________ 2021 
Norms Compliance Monitor  
____________ A. Omarova
â€œ___â€____________ 2021 



Almaty, 2021
MINISTRY OF EDUCATION AND SCIENCE OF THE REPUBLIC OF
KAZAKHSTAN
â€œKazakh-British Technical Universityâ€ JSC
Faculty of Information Technology
Speciality 5B070300 â€œInformation Systemsâ€
ADMITTED TO DEFENCE
Dean of FIT,
Associate Professor, PhD
___________ R. Suliyev
â€œ___â€___________ 2021

                                
DIPLOMA PROJECT ASSIGNMENT
Students: Abzal Myrzash
Project title: â€œAutomatic summarization of articlesâ€
Approved by the KBTU order: â„– _______dated â€œ___â€ ___________         
Submission deadline: ____________________
List of issues addressed in the diploma project or its brief content:
     1. Over the course of the diploma project, the following objectives were set and accomplished:
â€¢	Research related projects carried out in the area of Natural Language Processing and Text Summarization;
â€¢	Build a dataset with documents in English to summarize;
â€¢	Analyze and pre-process linguistic parameters that affect key word extraction and summarization;
â€¢	Implement extractive and abstractive text summarization algorithms;
â€¢	Test and compare the algorithms using various metrics for evaluating the quality of computer-generated summary. ROUGE results	
CONCLUSION	
REFERENCES	
APPENDIX	
      Code for Word Frequency algorithm, v0.1.0	
      Code for single-document TF-IDF algorithm, v0.2.0	
      Code for multi-document TF-IDF algorithm, v0.2.1	
      Code for TextRank algorithm, v0.3.0	
      Code for TextRank algorithm with USE, v.0.3.1	
      Code for Latent Semantic Analysis, v.0.4.0	




















INTRODUCTION
     In the age where technology has evolved to the point that one can share large amounts of information with millions of people all around the world in a matter of milliseconds, there is an enormous amount of unstructured textual material, including news articles, books, journals, web pages, being written every day. Therefore, the highest priority task in an extractive approach is selecting the right sentences or words for summarization. While in the party, Elizabeth collapsed and was rushed to the hospital.â€
an extractive summarizer would look for the most meaningful words and combine them into a shorter summary. Elizabeth rushed hospital.â€
     In practice, most articles contain much more sentences than that, so most extractive algorithms look for entire sentences instead of singular words. Abstractive summarizer
     Using the same source text example from extractive summarization as input, an abstractive summarizer would paraphrase some of the words and produce a summary resembling:
â€œElizabeth was hospitalized after attending a party with Peter.â€
     The summarizer did not only correctly identify the main idea, but also rephrased â€œrushed to the hospitalâ€ to â€œhospitalizedâ€, â€œPeter and Elizabeth â€¦ attend the night party â€¦â€ to â€œattending a party with Peterâ€ and compressed two sentences into one sentence. Finding those key words in an unprocessed text is a difficult task for computers, hence input text first needs to be properly pre-processed to summarize effectively. The pre-processing stage of text summarization includes, but is not limited to, cutting out unwanted parts of a text, converting the words to their most basic forms, and separating them using various Natural Language Processing techniques which will be described below. Stop words in the English language include function words, such as articles (â€œaâ€, â€œanâ€, â€œtheâ€), pronouns (e.g. â€œatâ€, â€œonâ€, â€œinâ€), and even some common lexical words (e.g. The concept of removing stop words is frequently used in search engine optimization, but they are as important in extractive summarization. Since key words in most of such algorithms are determine by their frequency, the algorithm may select sentences with the most stop words instead of the most meaningful sentences to generate a summary if we do not remove them. Removing punctuation is also critical because punctuation may interfere with stemming, lemmatization, and word and sentence tokenization, and even determining stop words. Tokenization thefore will be classified into 3 types â€“ sentence, word, and subword (n-gram) tokenization. The code of text summarization algorithms in this study will explicitly use only sentence and word tokenization, but that does not mean subword tokenization does not find its use implicitly (e.g. Word tokenization is employed in both extractive and abstractive summarization algorithms, to extract key words and generate a vocabulary of a document or a collection of documents. For instance, the sentence:
â€œIâ€™ve lived in the U.S.A. for 5 years.â€
 will be separated into 4 meaningless sentences:
1) â€œIâ€™ve lived in the U.â€; 2) â€œS.â€; 3) â€œA.â€; 4) â€œfor 5 years.â€
if we simply split the text by full stops. Stemming and lemmatization
    For grammatical correctness, sentences are going to contain various forms of the same word, such as â€œuseâ€, â€œusesâ€, and â€œusingâ€. Furthermore, families of words with slightly different meanings can be derived from just one root word, for example: â€œdigitâ€, â€œdigitalâ€, and â€œdigitalizeâ€. For instance, the rule:
 
maps the word â€œmovementâ€ to â€œmovâ€, but not â€œcementâ€ to â€œcâ€. Word Frequency
     Word Frequency algorithm is one of the most basic extractive summarization algorithms, and also makes up the first version of the authorâ€™s text summarization program. Word Frequency algorithm flowchart
     After the pre-processing stage of summarization, including steps such as tokenization, stemming, and stop word removal, is done, our main algorithm first calculates the weight of each unique word by counting the number of occurrences of that word (note: this also counts words that are different but have the same root, since all of the words were stemmed or lemmatized in the previous stage) in the document. In the second step, the word weights are used to calculate the score of every sentence in the document. The more â€œimportantâ€ (higher-weighted) words a sentence has, the higher its score is, and the likelier it is to be included in the final summary. For example, the algorithm may choose widespread auxiliary words such as â€œcanâ€, â€œhaveâ€, â€œhoweverâ€ over the word â€œintelligenceâ€ in an article about AI. It feeds on a sequence of sentences, words, characters or other tokens, encodes it, and then decodes it back in order to get a new sequence. In case of machine translation, the input would be original text, and the output is the same text translated to another language:
 
    For Named Entity Recognition (NER), a Seq2Seq model would feed on a sequence of words and generate a sequence of NER tags for every inputed word. During each timestep, a word is fed into the encoder one by one along with the previous timestepâ€™s internal memory and hidden states (if it is the first timestep, the states are randomly initialized). Examples of N-grams of words:
â€¢	Unigrams - car, refrigerator, bed, â€¦
â€¢	Bigrams - beautiful car, big refrigerator, comfotable bed, ...
â€¢	Trigrams â€“ I like football, long and tedious, ...
     Due to the fact that a language is not a random set of words, N-grams are a good characteristic of texts and language. Many natural language processing algorithms use N-gram as a base for:
â€¢	language detection: methods based on the use of N-gram letters give more precision (googlelangdetect);
â€¢	text generation: a sequence of N-grams such that the end of the i-th N-gram is the beginning of i + 1 N-grams is syntactically related text;
â€¢	searching for semantic errors: if a word is not used in a given context, then the probability of encountering an N-gram containing this word in this context will be small (or 0), which allows us to conclude about a semantic error. However, the longest of them is â€œriding a bicycle and reading books in myâ€, which is 8 words long. Let ğ‘‹ be a sequence of words of an exemplary abstract of length ğ‘›, ğ‘Œ be a sequence of words of an automatic abstract of length ğ‘š, ğ¿ğ¶ğ‘† (ğ‘‹, ğ‘Œ) is the length of the longest common subsequence between ğ‘‹ and ğ‘Œ. Let ğ‘†ğ¾ğ¼ğ‘ƒ2(ğ‘‹, ğ‘Œ) be the number of bigrams with gaps that occur both in a model abstract ğ‘‹ of length ğ‘› (words) and in an automatic abstract ğ‘Œ of length ğ‘š. Manual analysis
     For manual analysis of computer-generated summaries, extractive algorithms (Word Frequency, TF-IDF, TextRank, LSA) were performed on the following source text:
 
Figure x. Sample source text about resilience, part 2
     The article contains 58 sentences and over 1000 words. Meanwhile, Word Frequency algorithm is probably best suited for online text summarization web applications, where thousands of requests are being processed by relatively small servers.