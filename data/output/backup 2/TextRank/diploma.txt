Over the course of the diploma project, the following objectives were set and accomplished:
•	Research related projects carried out in the area of Natural Language Processing and Text Summarization;
•	Build a dataset with documents in English to summarize;
•	Analyze and pre-process linguistic parameters that affect key word extraction and summarization;
•	Implement extractive and abstractive text summarization algorithms;
•	Test and compare the algorithms using various metrics for evaluating the quality of computer-generated summary. Extractive text summarization methods generate a summary by extracting several main parts of the source text, such as sentences or phrases with the most important keywords. Therefore, the highest priority task in an extractive approach is selecting the right sentences or words for summarization. Elizabeth rushed hospital.”
     In practice, most articles contain much more sentences than that, so most extractive algorithms look for entire sentences instead of singular words. Abstractive summarizer
     Using the same source text example from extractive summarization as input, an abstractive summarizer would paraphrase some of the words and produce a summary resembling:
“Elizabeth was hospitalized after attending a party with Peter.”
     The summarizer did not only correctly identify the main idea, but also rephrased “rushed to the hospital” to “hospitalized”, “Peter and Elizabeth … attend the night party …” to “attending a party with Peter” and compressed two sentences into one sentence. Finding those key words in an unprocessed text is a difficult task for computers, hence input text first needs to be properly pre-processed to summarize effectively. The pre-processing stage of text summarization includes, but is not limited to, cutting out unwanted parts of a text, converting the words to their most basic forms, and separating them using various Natural Language Processing techniques which will be described below. Stop words in the English language include function words, such as articles (“a”, “an”, “the”), pronouns (e.g. “at”, “on”, “in”), and even some common lexical words (e.g. “be”, “want”, “have”)[2]. The concept of removing stop words is frequently used in search engine optimization, but they are as important in extractive summarization. Since key words in most of such algorithms are determine by their frequency, the algorithm may select sentences with the most stop words instead of the most meaningful sentences to generate a summary if we do not remove them. Depending on the task, these tokens can be sentences, words, characters, and subwords. Tokenization thefore will be classified into 3 types – sentence, word, and subword (n-gram) tokenization. The code of text summarization algorithms in this study will explicitly use only sentence and word tokenization, but that does not mean subword tokenization does not find its use implicitly (e.g. [3]
     Sentence tokenization will be implemented mostly in extractive summarization algorithms, because to determine which sentences to select for a summary, the computer needs to first understand the boundaries of each sentence. Word tokenization is employed in both extractive and abstractive summarization algorithms, to extract key words and generate a vocabulary of a document or a collection of documents. For instance, the sentence:
“I’ve lived in the U.S.A. for 5 years.”
 will be separated into 4 meaningless sentences:
1) “I’ve lived in the U.”; 2) “S.”; 3) “A.”; 4) “for 5 years.”
if we simply split the text by full stops. Stemming and lemmatization
    For grammatical correctness, sentences are going to contain various forms of the same word, such as “use”, “uses”, and “using”. Furthermore, families of words with slightly different meanings can be derived from just one root word, for example: “digit”, “digital”, and “digitalize”. Since the verb “operate” and its derived forms are common and have different meanings based on context, a stemmer could lose important information in phrases such as:
operational research
operating system
operatived dentistry
     For such cases, a lemmatizer would be more useful than stemming, since it removes only inflectional endings (e.g. Word Frequency
     Word Frequency algorithm is one of the most basic extractive summarization algorithms, and also makes up the first version of the author’s text summarization program. Word Frequency algorithm flowchart
     After the pre-processing stage of summarization, including steps such as tokenization, stemming, and stop word removal, is done, our main algorithm first calculates the weight of each unique word by counting the number of occurrences of that word (note: this also counts words that are different but have the same root, since all of the words were stemmed or lemmatized in the previous stage) in the document. In the second step, the word weights are used to calculate the score of every sentence in the document. The more “important” (higher-weighted) words a sentence has, the higher its score is, and the likelier it is to be included in the final summary. To implement that, we will tokenize the sentence into words, look up each of those words’ weights from freqTable, and simply calculate their sum. Finally, to generate the summary, we will calculate the average sentence score, and multiply it with a certain coefficient (for example, 1.5) to use as a minimum score threshold for a summary sentence. After that, sentence scores will be calculated as the sum of TF-IDF values of words in the sentence. The PageRank algorithm calculates the weights of each page using this formula: [8]
 
     TextRank works the exact same way, except the nodes are sentences instead of pages and their edges are their mutual similarity instead of links or references. Sequence-to-Sequence Modeling
    A Sequence-to-Sequence (Seq2Seq) model is a machine learning model most commonly used in language process applications such as neural machine translation, named entity recognition, sentiment classification, and text summarization. It feeds on a sequence of sentences, words, characters or other tokens, encodes it, and then decodes it back in order to get a new sequence. In case of machine translation, the input would be original text, and the output is the same text translated to another language:
 
    For Named Entity Recognition (NER), a Seq2Seq model would feed on a sequence of words and generate a sequence of NER tags for every inputed word. During each timestep, a word is fed into the encoder one by one along with the previous timestep’s internal memory and hidden states (if it is the first timestep, the states are randomly initialized). Many natural language processing algorithms use N-gram as a base for:
•	language detection: methods based on the use of N-gram letters give more precision (googlelangdetect);
•	text generation: a sequence of N-grams such that the end of the i-th N-gram is the beginning of i + 1 N-grams is syntactically related text;
•	searching for semantic errors: if a word is not used in a given context, then the probability of encountering an N-gram containing this word in this context will be small (or 0), which allows us to conclude about a semantic error. Manual analysis
     For manual analysis of computer-generated summaries, extractive algorithms (Word Frequency, TF-IDF, TextRank, LSA) were performed on the following source text:
 
Figure x. Sample source text about resilience, part 2
     The article contains 58 sentences and over 1000 words. Meanwhile, Word Frequency algorithm is probably best suited for online text summarization web applications, where thousands of requests are being processed by relatively small servers.